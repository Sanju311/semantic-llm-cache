# semantic-llm-cache
A high-performance semantic caching layer for LLM applications designed to minimize redundant LLM calls, built with FastAPI and Redis, featuring exact and embedding-based cache reuse with smart, query-aware TTLs.
